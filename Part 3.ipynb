{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x2a6ebb7b5f8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load('300features_40minwords_10text')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word2vec 모델은 어휘의 각 단어에 대한 feature vector로 구성, syn0이라는 np배열로 저장(행 수는 모델 어휘의 수)\n",
    "type(model.wv.syn0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11986, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape #행은 모델 어휘의 단어 수, 열은 특징 벡터의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['flower'].shape #개별 단어 벡터 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06458191, -0.15343866, -0.01922338, -0.07924391,  0.07760002,\n",
       "       -0.0630917 ,  0.09446166,  0.02681448, -0.02802669, -0.00462039], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['flower'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K-Means</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for k-means clustering:  185.98822832107544 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "word_vectors = model.wv.syn0 #어휘의 특징백터\n",
    "num_clusters = word_vectors.shape[0] / 5 #k값은 어휘 크기의 1/5이나 평균 5단어로 설정\n",
    "num_clusters = int(num_clusters)\n",
    "\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print('time taken for k-means clustering: ', elapsed, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['manuscript', 'journal', 'letter']\n",
      "\n",
      "Cluster 1\n",
      "['scruffi']\n",
      "\n",
      "Cluster 2\n",
      "['theo', 'malik', 'trey', 'fenton', 'platt', 'ditto']\n",
      "\n",
      "Cluster 3\n",
      "['tent', 'hut']\n",
      "\n",
      "Cluster 4\n",
      "['grit', 'testosteron', 'fill', 'brim', 'crackl']\n",
      "\n",
      "Cluster 5\n",
      "['serv', 'reinforc', 'apathi']\n",
      "\n",
      "Cluster 6\n",
      "['repetiti', 'repetit', 'occasion', 'overus', 'tire', 'jar', 'tiresom']\n",
      "\n",
      "Cluster 7\n",
      "['elisabeth', 'linda', 'fletcher', 'paquin', 'melissa', 'deneuv', 'bacal', 'nanci', 'kristin', 'sue', 'paig', 'kati', 'vicki', 'gina', 'selma', 'catherin', 'natasha', 'bateman', 'gwen', 'lisa', 'dian', 'bianca', 'sarandon', 'katherin', 'nicol', 'julia', 'becki', 'samantha', 'ami', 'stile', 'cathi', 'nina', 'hannah', 'fari', 'jill', 'susi', 'beth', 'mandi', 'debbi', 'gyllenha', 'drew', 'tammi', 'friel', 'melani', 'tina', 'lindsey', 'sara', 'morton', 'kudrow', 'hershey', 'megan', 'fiorentino', 'donna', 'zoe', 'daryl', 'lauren', 'christin', 'amanda', 'chelsea', 'bennet', 'dane', 'dickinson', 'karen', 'tilli', 'jenni', 'gail', 'heather']\n",
      "\n",
      "Cluster 8\n",
      "['usa', 'u', 'uk', 'britain', 'oversea', 'america']\n",
      "\n",
      "Cluster 9\n",
      "['goug', 'twitch', 'gape', 'candi', 'throb', 'bulg']\n"
     ]
    }
   ],
   "source": [
    "#각 어휘 단어를 클러스터 번호에 매핑되게 word / index 사전을 만든다\n",
    "idx = list(idx)\n",
    "names = model.wv.index2word\n",
    "word_centroid_map = {names[i]: idx[i] for i in range(len(names))}\n",
    "\n",
    "for cluster in range(10): #첫번째 클러스터에서 처음 10개를 찍어본다. -> 비슷한 단어가 군집화된것을 확인\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    \n",
    "    #클러스터 번호와 클러스터에 있는 단어를 찍는다\n",
    "    words = []\n",
    "    for i in range(0, len(list(word_centroid_map.values()))):\n",
    "        if(list(word_centroid_map.values())[i] == cluster):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/labeledTrainData.tsv', \n",
    "                    header=0, delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv('data/testData.tsv', \n",
    "                   header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocessing\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(preprocessing.review_to_wordlist(review, remove_stopwords = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test['review']:\n",
    "    clean_test_reviews.append(preprocessing.review_to_wordlist(review, remove_stopwords = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bags of centroids 생성\n",
    "train_centroids = np.zeros((train['review'].size, num_clusters), dtype='float32')\n",
    "train_centroids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroid는 두 클러스터의 중심점을 정의 한 다음 중심점의 거리를 측정한 것\n",
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    num_centroids = max(word_centroid_map.values()) + 1\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype = 'float32')\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "            \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_centroids = np.zeros((test['review'].size, num_clusters), dtype='float32')\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random-forest to labeled training data...\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "print(\"Fitting a random-forest to labeled training data...\")\n",
    "%time forest = forest.fit(train_centroids, train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "%time score = np.mean(cross_val_score(forest, train_centroids, train['sentiment'], cv=10, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%time result = forest.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91289043199999986"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"data/submit_BagOfCentroids_{0:.5f}.csv\".format(score), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    12833\n",
       "1    12167\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentiment = output['sentiment'].value_counts()\n",
    "print(output_sentiment[0] - output_sentiment[1])\n",
    "output_sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
